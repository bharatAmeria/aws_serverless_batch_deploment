1. Create repo, clone it in local
2. Create Python virtual environement -> uv venv --python 3.10 .venv (Install using uv. If not installed -> pip install uv)
    (If you are making virtual environement with other name. Make sure to add it into .gitignore file.)
    2.1. Activate the environement -> source .venv/bin/activate
    2.2. Install packages using -> uv add requests pandas
    2.3. Install dependencies -> uv sync --frozen or uv sync 
    2.4. See libraries and its dependencies -> uv tree
    2.5. Run Python file using -> uv run <file_name>
    2.6. Remove Python flask -> uv remove <package_name>
3. Copy config.json Paste in the root directory.
4. Copy pyproject.toml file. Save it in your directory and Change author name accordingly.
5. Copy and paste src folder in your project directory.
6. Now configure our src folder as a package using -> uv pip install -e .
7. Create .env file and add 
    AWS_ACCESS_KEY_ID=
    AWS_SECRET_ACCESS_KEY=
    AWS_S3_BUCKET_NAME=
    AWS_DEFAULT_REGION=
    DATASET_URI=https://drive.google.com/file/d/17nz2lVYeLqbCWDuZb2h2gpROe5BJOe2K/view?usp=sharing

8. Now SignIn in your aws google console create an IAM user and give administrator access.
    Download AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION.

    Add these credentials to .env file.
9. Now do { aws configure }. It takes AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION as input.
10. Now create s3 bucket.  
    aws s3 mb s3://learnyard-data-ingestion --region eu-north-1

    Add bucket name in the .env file AWS_S3_BUCKET_NAME
11. Now create s3 bucket.  
    aws s3 mb s3://learnyard-data-ingestion --region eu-north-1
12. Copy main.py file project root directory. Run file main.py to check if components are working or not.
    Add bucket name in the .env file AWS_S3_BUCKET_NAME


13. Make folder -> mkdir lambda_function
                   cd lambda_function

14. Copy app.py, rquirements.txt, dockerfile and paste in the lambda_function folder.
15. Now build Docker image for our fastApi app.  
    a. Create an ECR repo where we push our image -> aws ecr create-repository --repository-name lambda_batch_inference
    b. Now go on AWS console and seacrh for ECR.
    c. Check ECR repo is created or not.
    d. Now move in the repo and On the top right click on the  icon named (view oush command)
       (This helps you to login in your private repo and push the images in repo)
    e. Follow command step by step.

16. Test Docker Image Locally -> 
      docker run -p 9000:8080 -e AWS_ACCESS_KEY_ID= <  > -e AWS_SECRET_ACCESS_KEY= <  > -e AWS_DEFAULT_REGION= <  > 684931212328.dkr.ecr.eu-north-1.amazonaws.com/batch_prediction_lambda:latest

17.  invoke using -> curl -XPOST "http://localhost:9000/2015-03-31/functions/function/invocations" -d '{}'
      (it should give respone 200)

18. Iam s3 role permission for lambda_function
       (Create file name trust-policy.json)
       aws iam create-role --role-name lambda-s3-role --assume-role-policy-document file://trust-policy.json
       aws iam attach-role-policy --role-name lambda-s3-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess

19. Create lambda_function using below Command ->

aws lambda create-function --function-name batch_prediction_lambda --package-type Image --code ImageUri=684931265998.dkr.ecr.eu-north-1.amazonaws.com/batch_prediction_lambda:latest --role arn:aws:iam::684931265998:role/lambda-s3-role 
  --timeout 300 \
  --memory-size 1024

20. Invoke lambda function -> 
    aws lambda invoke \
    --function-name batch_prediction_lambda \
    --payload '{}' \
    response.json
    cat response.json

(Should return a status 200 and the S3 path:)

21. Copy .github folder. (To automate the whole Process till pushing our model to S3 bucket.)

---------------------------------------

aws ecr delete-repository \
  --repository-name batch_prediction_lambda \
  --force

aws lambda delete-function --function-name batch_prediction_lambda

# Delete all objects in the bucket
aws s3 rm s3://learnyard-data-ingestion --recursive

# Delete the bucket
aws s3api delete-bucket \
  --bucket learnyard-data-ingestion \
  --region us-east-1


